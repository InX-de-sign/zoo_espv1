# Orchestrating all services together
# version: '3.8'

services:
  # Chatbot Service (FastAPI)
  chatbot:
    build: 
      context: ./chatbot
      dockerfile: Dockerfile
    container_name: museum_chatbot
    ports:
      - "0.0.0.0:8000:8000"
    environment:
      - DATABASE_URL=postgresql://museum_user:museum_pass@postgres:5432/museum_db
      - REDIS_URL=redis://redis:6379
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - CV_SERVICE_URL=http://yolo_inference:5000
      - HOST=0.0.0.0
    depends_on:
      - postgres
      - redis
    volumes:
      - ./chatbot:/app
      - chatbot_logs:/app/logs
    networks:
      - museum_network
    restart: unless-stopped
    command: python zoo_api.py
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # YOLO Inference Service - Receives frames from Raspberry Pi webcam_client
  yolo_inference:
    build:
      context: ./inference
      dockerfile: Dockerfile
    container_name: yolo_inference
    ports:
      - "5000:5000"
    environment:
      - MODEL_PATH=/app/models/best.onnx
      - CHATBOT_URL=http://museum_chatbot:8000
      # REMOVED: CAMERA_URL - we receive frames via /predict/ instead
      - INFERENCE_INTERVAL=0.5
      - DETECTION_COOLDOWN=5
      # Detection parameters (matching your working local setup)
      - IMGSZ=640
      - CONF_THRESHOLD=0.75
      - DEVICE=cpu
    volumes:
      - ./inference:/app
    working_dir: /app
    command: bash -c "apt update && apt install -y libgl1 && pip install -r requirements.txt && python -m uvicorn model_api:app --host 0.0.0.0 --port 5000"
    depends_on:
      - chatbot
    networks:
      - museum_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # PostgreSQL Database
  postgres:
    image: postgres:13
    container_name: museum_postgres
    environment:
      POSTGRES_DB: museum_db
      POSTGRES_USER: museum_user
      POSTGRES_PASSWORD: museum_pass
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    networks:
      - museum_network
    restart: unless-stopped

  # Redis for caching
  redis:
    image: redis:6-alpine
    container_name: museum_redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - museum_network
    restart: unless-stopped
    command: redis-server --appendonly yes

  # Vector Database (for embeddings)
  qdrant:
    image: qdrant/qdrant:latest
    container_name: museum_vectordb
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage
    networks:
      - museum_network
    restart: unless-stopped

    # MediaMTX - RTSP Streaming Server
  mediamtx:
    image: bluenviron/mediamtx:latest
    container_name: mediamtx
    ports:
      - "8554:8554"  # RTSP
      - "8888:8888"  # HLS
      - "1935:1935"  # RTMP
    volumes:
      - ./mediamtx.yml:/mediamtx.yml
    networks:
      - museum_network
    restart: unless-stopped

# Define volumes
volumes:
  postgres_data:
  redis_data:
  qdrant_data:
  chatbot_logs:

# Define network
networks:
  museum_network:
    driver: bridge